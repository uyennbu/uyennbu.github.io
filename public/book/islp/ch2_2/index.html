<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Chapter 2: Introduction to Statistical Learning (2) | Uyen&#39;s blog</title>
<meta name="keywords" content="statistics, machine learning">
<meta name="description" content="Bài viết nằm trong series ISLP, là series mình tóm tắt lại những gì mình đọc trong cuốn &ldquo;An Introduction to Statistical Learning with applications in Python&rdquo;.

Trang web của quyển sách: statlearning.com
Trang resources để tải file sách pdf, tải code và data:  resources
Bài giảng của tác giả được cung cấp bởi đại học Stanford: Statistical Learning with Python

2.2 Đánh giá sự chính xác
Chúng ta cần nhiều phương pháp thay vì một cách duy nhất vì không có model nào là phù hợp với tất cả các tập dữ liệu. Với một tập dữ liệu nhất định có thể phù hợp với phương pháp này, nhưng một tập dữ liệu tương tự khác lại biểu hiện tốt hơn với phương pháp khác. Chọn cách tiếp cận đúng có lẽ là một trong những nhiệm vụ quan trọng nhất trên thực tế.">
<meta name="author" content="">
<link rel="canonical" href="https://uyennbu.github.io/book/islp/ch2_2/">
<link crossorigin="anonymous" href="https://uyennbu.github.io/assets/css/stylesheet.da3211e5ef867bf2b75fd5a6515cfed7195c011e8ab735694e203810a827097b.css" integrity="sha256-2jIR5e&#43;Ge/K3X9WmUVz&#43;1xlcAR6KtzVpTiA4EKgnCXs=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://uyennbu.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://uyennbu.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://uyennbu.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://uyennbu.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://uyennbu.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://uyennbu.github.io/book/islp/ch2_2/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"></script>

<script>
  MathJax = {
    tex: {
      displayMath: [['\[', '\]'], ['$$', '$$']],  
      inlineMath: [['$', '$']]                  
    },
    loader:{
      load: ['ui/safe']
    },
  };
</script>
<meta property="og:url" content="https://uyennbu.github.io/book/islp/ch2_2/">
  <meta property="og:site_name" content="Uyen&#39;s blog">
  <meta property="og:title" content="Chapter 2: Introduction to Statistical Learning (2)">
  <meta property="og:description" content="Bài viết nằm trong series ISLP, là series mình tóm tắt lại những gì mình đọc trong cuốn “An Introduction to Statistical Learning with applications in Python”.
Trang web của quyển sách: statlearning.com Trang resources để tải file sách pdf, tải code và data: resources Bài giảng của tác giả được cung cấp bởi đại học Stanford: Statistical Learning with Python 2.2 Đánh giá sự chính xác Chúng ta cần nhiều phương pháp thay vì một cách duy nhất vì không có model nào là phù hợp với tất cả các tập dữ liệu. Với một tập dữ liệu nhất định có thể phù hợp với phương pháp này, nhưng một tập dữ liệu tương tự khác lại biểu hiện tốt hơn với phương pháp khác. Chọn cách tiếp cận đúng có lẽ là một trong những nhiệm vụ quan trọng nhất trên thực tế.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="book">
    <meta property="article:published_time" content="2026-01-17T00:00:00+00:00">
    <meta property="article:modified_time" content="2026-01-17T00:00:00+00:00">
    <meta property="article:tag" content="Statistics">
    <meta property="article:tag" content="Machine Learning">
      <meta property="og:see_also" content="https://uyennbu.github.io/book/islp/ch2_1/">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 2: Introduction to Statistical Learning (2)">
<meta name="twitter:description" content="Bài viết nằm trong series ISLP, là series mình tóm tắt lại những gì mình đọc trong cuốn &ldquo;An Introduction to Statistical Learning with applications in Python&rdquo;.

Trang web của quyển sách: statlearning.com
Trang resources để tải file sách pdf, tải code và data:  resources
Bài giảng của tác giả được cung cấp bởi đại học Stanford: Statistical Learning with Python

2.2 Đánh giá sự chính xác
Chúng ta cần nhiều phương pháp thay vì một cách duy nhất vì không có model nào là phù hợp với tất cả các tập dữ liệu. Với một tập dữ liệu nhất định có thể phù hợp với phương pháp này, nhưng một tập dữ liệu tương tự khác lại biểu hiện tốt hơn với phương pháp khác. Chọn cách tiếp cận đúng có lẽ là một trong những nhiệm vụ quan trọng nhất trên thực tế.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Books",
      "item": "https://uyennbu.github.io/book/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Chapter 2: Introduction to Statistical Learning (2)",
      "item": "https://uyennbu.github.io/book/islp/ch2_2/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Chapter 2: Introduction to Statistical Learning (2)",
  "name": "Chapter 2: Introduction to Statistical Learning (2)",
  "description": "Bài viết nằm trong series ISLP, là series mình tóm tắt lại những gì mình đọc trong cuốn \u0026ldquo;An Introduction to Statistical Learning with applications in Python\u0026rdquo;.\nTrang web của quyển sách: statlearning.com Trang resources để tải file sách pdf, tải code và data: resources Bài giảng của tác giả được cung cấp bởi đại học Stanford: Statistical Learning with Python 2.2 Đánh giá sự chính xác Chúng ta cần nhiều phương pháp thay vì một cách duy nhất vì không có model nào là phù hợp với tất cả các tập dữ liệu. Với một tập dữ liệu nhất định có thể phù hợp với phương pháp này, nhưng một tập dữ liệu tương tự khác lại biểu hiện tốt hơn với phương pháp khác. Chọn cách tiếp cận đúng có lẽ là một trong những nhiệm vụ quan trọng nhất trên thực tế.\n",
  "keywords": [
    "statistics", "machine learning"
  ],
  "articleBody": "Bài viết nằm trong series ISLP, là series mình tóm tắt lại những gì mình đọc trong cuốn “An Introduction to Statistical Learning with applications in Python”.\nTrang web của quyển sách: statlearning.com Trang resources để tải file sách pdf, tải code và data: resources Bài giảng của tác giả được cung cấp bởi đại học Stanford: Statistical Learning with Python 2.2 Đánh giá sự chính xác Chúng ta cần nhiều phương pháp thay vì một cách duy nhất vì không có model nào là phù hợp với tất cả các tập dữ liệu. Với một tập dữ liệu nhất định có thể phù hợp với phương pháp này, nhưng một tập dữ liệu tương tự khác lại biểu hiện tốt hơn với phương pháp khác. Chọn cách tiếp cận đúng có lẽ là một trong những nhiệm vụ quan trọng nhất trên thực tế.\n2.2.1 Đo lường độ fit Trong các bài toán hồi quy, để đo lường xem sự dự đoán của model thực sự gần với dữ liệu thực tế như thế nào, phương pháp thường được dùng nhiều nhất là bình phương tối thiểu (mean squared error) (MSE): $$MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\hat{f}(x_i))^2$$ Trong đó $\\hat f(x_i)$ là dự đoán mà $\\hat f$ đưa ra ứng với quan sát thứ i. Dự đoán càng gần với thực tế thì MSE càng nhỏ và ngược lại.\nMSE được tính theo công thức trên là tính dựa trên tập train, gọi là training MSE. Tuy nhiên, mục tiêu cuối cùng của ta không phải là làm cho training MSE nhỏ nhất, mà là làm cho test MSE nhỏ nhất. Tức là độ chính xác là cao nhất khi chúng ta sử dụng model để dự đoán giá trị trong tập test, những giá trị mà không có trong tập huấn luyện ban đầu. Nếu ta có một lượng lớn các quan sát trong tập test, ta có thể tính: $$\\text{Ave}(y_0 + \\hat f(x_0))^2$$ sai số trung bình bình phương dự đoán cho những điểm test $(x_0, y_0)$ và chọn model cho ra giá trị này nhỏ nhất.\nTrên thực tế, ta chỉ có một tập dữ liệu duy nhất. Thay vì dùng toàn bộ để train mô hình, ta chia dữ liệu thành các tập train và test khác nhau. Có nhiều phương pháp để làm điều này, sẽ được nhắc đến ở phần sau. Một ví dụ là cross validation.\nOverfitting là khi mô hình quá khớp với dữ liệu train, do đó không còn đúng với mối quan hệ thực sự giữa $X$ và $Y$. Lúc này, một mô hình đơn giản hơn lại thể hiện tốt hơn một mô hình quá phức tạp.\nTrong hình các sau hàm $f$ thực tế là đường màu đen, đường màu đen là đường hồi quy tuyến tính, hai đường màu xanh là $\\hat f$ với độ phức tạp cao hơn. Ở hình trên, có thể thấy đường hồi quy quá đơn giản so với $f$ thực tế. Trong khi ở hình thứ 2 này, mô hình hồi quy tuyến tính lại làm rất tốt so với các mô hình phức tạp hơn: Một ví dụ khác: 2.2.2 Sự đánh đổi giữa bias và variance Kỳ vọng của test MSE (expected test MSE) tại điểm $x_0$: $$E(y_0 - \\hat f(x_0))^2 = \\text{Var}(\\hat f(x_0)) + [\\text{Bias}(\\hat f(x_0))]^2 + \\text{Var}(\\epsilon)$$ Để làm giảm $$E(y_0 - \\hat f(x_0))^2$$ thì ta cần chọn phương pháp mà có phương sai thấp (low variance) và bias thấp (low bias).\nVariance là sự thay đổi của $\\hat f$ nếu ta ước lượng nó bằng một tập dữ liệu training khác. Ước lượng cho $\\hat f$ không nên thay đổi quá nhiều khi thay đổi các tập train. Nếu một phương pháp có variance cao thì sự thay đổi nhỏ trong tập dữ liệu huấn luyện cũng làm $\\hat f$ thay đổi đáng kể. Các phương pháp có độ phức tạp flexible càng cao thì phương sai càng lớn.\nBias là lỗi xảy ra khi xấp xỉ một mô hình phức tạp bằng một mô hình quá đơn giản. Ví dụ, khi ta ước lượng một mối quan hệ phi tuyến (như $Y = X^2$ chẳng hạn) bằng một hàm tuyến tính $\\hat f$.\nKhi độ phức tạp càng cao, bias càng nhỏ còn variance càng tăng, $MSE$ có thể sẽ giảm xuống. Đến một mốc nào đó việc tăng độ phức tạp có ảnh hưởng rất nhỏ đến Bias nhưng lại làm tăng đáng kể variance, khiến MSE tăng. Hình trên cho thấy sự thay đổi của MSE test, bias, variance khi độ phức tạp tăng lên trong 3 trường hợp ở phần trên theo thứ tự tương ứng. 2.2.3 Tiếp cận cho bài toán phân loại Trong bài toán phân loại, tập dữ liệu huấn luyện của ta là $\\{(x_1, y_1), ..., (x_n, y_n)\\}$ trong đó $y_1, ..., y_n$ là các giá trị định tính. Cách phổ biến nhất để đánh giá độ chính xác của $\\hat f$ là tỉ lệ lỗi huấn luyện training error rate: $$\\frac{1}{n}\\sum_{i=1}^{n}I(y_i \\neq \\hat{y_i})$$ trong đó $\\hat {y_i}$ là nhãn phân loại dự đoán cho quan sát thứ $i$ sử dụng $\\hat f$. $I(y_i \\neq \\hat{y_i})$ là indicator variable, bằng 1 nếu $y_i = \\hat{y_i}$, bằng 0 nếu $y_i \\neq \\hat{y_i}$. Công thức trên dùng để tính training error\nTest error được tính qua tập test: $$\\text{Ave}(I(y_i \\neq \\hat{y_i}))$$Một mô hình phân loại tốt là mô hình có test error nhỏ nhất.\nBayes classifier Bayes classifier đưa ra dự đoán $y$ thuộc lớp $j$ dựa trên vector input $x_0$ mà xác suất: $$\\text{Pr}(Y=j|X=x_0)$$ là lớn nhất. Đây là xác suất có điều kiện: xác suất $Y = j$ biết rằng $X = x_0$.\nBayes classifier có test error rate nhỏ nhất có thể, gọi là Bayes error rate, thường được tính qua công thức: $$1 - E(\\max_{j}\\text{Pr}(Y=j|X))$$ Bayes error rate giống với sai số không thể giảm được irreducible error\nK-nearest neighbor Về mặt lý thuyết, ta luôn muốn sử dụng Bayes classifier. Tuy nhiên trên thực tế, ta không biết phân phối xác suất có điều kiện của Y theo X vì vậy tính Bayes classifier là không thể. Một trong những biện pháp thay thế là K-nearest neighbors (KNN) classifier.\nVới một số thực K \u003e 0 cho trước và một điểm test $x_0$. KNN xác định tập $\\mathcal{N}_0$ gồm K điểm trong tập train gần $x_0$ nhất sau đó tính: $$\\text{Pr}(Y=j|X=x_0)=\\frac{1}{K}\\sum_{i \\in \\mathcal{N}_0}I(y_i=j)$$ KNN gán điểm test $x_0$ vào lớp có xác suất cao nhất tính theo công thức trên.\nMinh họa về bài toán phân loại (hai lớp là cam và xanh). Đường màu đen là đường danh giới dự đoán của KNN, đường màu tím là của Bayes classifier (Bayes decision boundary).\nKhi K=10 hai đường khá khớp với nhau:\nĐường K=1 quá phức tạp (flexible) trong khi đường K=100 quá đơn giản:\nSự thay đổi của train và test error rate khi K thay đổi (1/K tăng - tức K giảm ứng với việc tăng độ phức tạp của mô hình). Đường màu đen là Bayes error rate:\nCó thể thấy K tốt nhất ứng với 1/K vào khoảng 0.1\nBài viết của mình có thể còn nhiều thiếu sót, mình rất vui nếu được nhận góp ý từ bạn đọc để bài viết hoàn thiện và trở nên tốt hơn. Chúc bạn một ngày tốt lành ☘️\nEmail của mình : uyennguyen.nbu@gmail.com\n",
  "wordCount" : "1238",
  "inLanguage": "en",
  "datePublished": "2026-01-17T00:00:00Z",
  "dateModified": "2026-01-17T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://uyennbu.github.io/book/islp/ch2_2/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Uyen's blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://uyennbu.github.io/favicon.ico"
    }
  }
}
</script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://uyennbu.github.io/" accesskey="h" title="Uyen&#39;s blog (Alt + H)">Uyen&#39;s blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://uyennbu.github.io/series/" title="Series">
                    <span>Series</span>
                </a>
            </li>
            <li>
                <a href="https://uyennbu.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://uyennbu.github.io/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://uyennbu.github.io/about/" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://uyennbu.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://uyennbu.github.io/book/">Books</a></div>
    <h1 class="post-title entry-hint-parent">
      Chapter 2: Introduction to Statistical Learning (2)
    </h1>
    <div class="post-meta"><span title='2026-01-17 00:00:00 +0000 UTC'>January 17, 2026</span>&nbsp;·&nbsp;<span>6 min</span>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#22-%c4%91%c3%a1nh-gi%c3%a1-s%e1%bb%b1-ch%c3%adnh-x%c3%a1c" aria-label="2.2 Đánh giá sự chính xác">2.2 Đánh giá sự chính xác</a><ul>
                        
                <li>
                    <a href="#221-%c4%91o-l%c6%b0%e1%bb%9dng-%c4%91%e1%bb%99-fit" aria-label="2.2.1 Đo lường độ fit">2.2.1 Đo lường độ fit</a></li>
                <li>
                    <a href="#222-s%e1%bb%b1-%c4%91%c3%a1nh-%c4%91%e1%bb%95i-gi%e1%bb%afa-bias-v%c3%a0-variance" aria-label="2.2.2 Sự đánh đổi giữa bias và variance">2.2.2 Sự đánh đổi giữa bias và variance</a></li>
                <li>
                    <a href="#223-ti%e1%ba%bfp-c%e1%ba%adn-cho-b%c3%a0i-to%c3%a1n-ph%c3%a2n-lo%e1%ba%a1i" aria-label="2.2.3 Tiếp cận cho bài toán phân loại">2.2.3 Tiếp cận cho bài toán phân loại</a><ul>
                        
                <li>
                    <a href="#bayes-classifier" aria-label="Bayes classifier">Bayes classifier</a></li>
                <li>
                    <a href="#k-nearest-neighbor" aria-label="K-nearest neighbor">K-nearest neighbor</a>
                </li>
            </ul>
            </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>Bài viết nằm trong series ISLP, là series mình tóm tắt lại những gì mình đọc trong cuốn &ldquo;An Introduction to Statistical Learning with applications in Python&rdquo;.</p>
<ul>
<li>Trang web của quyển sách: <a href="https://www.statlearning.com/">statlearning.com</a></li>
<li>Trang resources để tải file sách pdf, tải code và data:  <a href="https://www.statlearning.com/resources-python">resources</a></li>
<li>Bài giảng của tác giả được cung cấp bởi đại học Stanford: <a href="https://youtube.com/playlist?list=PLoROMvodv4rPP6braWoRt5UCXYZ71GZIQ&amp;si=wvVcrbaPvFn4x9wb">Statistical Learning with Python</a></li>
</ul>
<h2 id="22-đánh-giá-sự-chính-xác">2.2 Đánh giá sự chính xác<a hidden class="anchor" aria-hidden="true" href="#22-đánh-giá-sự-chính-xác">#</a></h2>
<p>Chúng ta cần nhiều phương pháp thay vì một cách duy nhất vì không có model nào là phù hợp với tất cả các tập dữ liệu. Với một tập dữ liệu nhất định có thể phù hợp với phương pháp này, nhưng một tập dữ liệu tương tự khác lại biểu hiện tốt hơn với phương pháp khác. Chọn cách tiếp cận đúng có lẽ là một trong những nhiệm vụ quan trọng nhất trên thực tế.</p>
<h3 id="221-đo-lường-độ-fit">2.2.1 Đo lường độ fit<a hidden class="anchor" aria-hidden="true" href="#221-đo-lường-độ-fit">#</a></h3>
<p>Trong các bài toán hồi quy, để đo lường xem sự dự đoán của model thực sự gần với dữ liệu thực tế như thế nào, phương pháp thường được dùng nhiều nhất là bình phương tối thiểu (<em>mean squared error</em>) (MSE):
</p>
$$MSE = \frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{f}(x_i))^2$$<p>
Trong đó $\hat f(x_i)$ là dự đoán mà $\hat f$ đưa ra ứng với quan sát thứ i. Dự đoán càng gần với thực tế thì MSE càng nhỏ và ngược lại.</p>
<p>MSE được tính theo công thức trên là tính dựa trên tập train, gọi là <em>training MSE</em>. Tuy nhiên, mục tiêu cuối cùng của ta không phải là làm cho <em>training MSE</em> nhỏ nhất, mà là làm cho <em>test MSE</em> nhỏ nhất. Tức là độ chính xác là cao nhất khi chúng ta sử dụng model để dự đoán giá trị trong tập test, những giá trị mà không có trong tập huấn luyện ban đầu. Nếu ta có một lượng lớn các quan sát trong tập test, ta có thể tính:
</p>
$$\text{Ave}(y_0 + \hat f(x_0))^2$$<p>
sai số trung bình bình phương dự đoán cho những điểm test $(x_0, y_0)$ và chọn model cho ra giá trị này nhỏ nhất.</p>
<p>Trên thực tế, ta chỉ có một tập dữ liệu duy nhất. Thay vì dùng toàn bộ để train mô hình, ta chia dữ liệu thành các tập train và test khác nhau. Có nhiều phương pháp để làm điều này, sẽ được nhắc đến ở phần sau. Một ví dụ là <em>cross validation</em>.</p>
<p>Overfitting là khi mô hình quá khớp với dữ liệu train, do đó không còn đúng với mối quan hệ thực sự giữa $X$ và $Y$. Lúc này, một mô hình đơn giản hơn lại thể hiện tốt hơn một mô hình quá phức tạp.</p>
<p>Trong hình các sau hàm $f$ thực tế là đường màu đen, đường màu đen là đường hồi quy tuyến tính, hai đường màu xanh là $\hat f$ với độ phức tạp cao hơn.
<img alt="2.9" loading="lazy" src="https://uyennbu.github.io/img/2_9.jpg">
Ở hình trên, có thể thấy đường hồi quy quá đơn giản so với $f$ thực tế. Trong khi ở hình thứ 2 này, mô hình hồi quy tuyến tính lại làm rất tốt so với các mô hình phức tạp hơn:
<img alt="2.10" loading="lazy" src="https://uyennbu.github.io/img/2_10.jpg">
Một ví dụ khác:
<img alt="2.11" loading="lazy" src="https://uyennbu.github.io/img/2_11.jpg"></p>
<h3 id="222-sự-đánh-đổi-giữa-bias-và-variance">2.2.2 Sự đánh đổi giữa bias và variance<a hidden class="anchor" aria-hidden="true" href="#222-sự-đánh-đổi-giữa-bias-và-variance">#</a></h3>
<p>Kỳ vọng của test MSE (<em>expected test MSE</em>) tại điểm $x_0$:
</p>
$$E(y_0 - \hat f(x_0))^2 = \text{Var}(\hat f(x_0)) + [\text{Bias}(\hat f(x_0))]^2 + \text{Var}(\epsilon)$$<p>
Để làm giảm </p>
$$E(y_0 - \hat f(x_0))^2$$<p> thì ta cần chọn phương pháp mà có phương sai thấp (<em>low variance</em>) và bias thấp (<em>low bias</em>).</p>
<p><em>Variance</em> là sự thay đổi của $\hat f$ nếu ta ước lượng nó bằng một tập dữ liệu training khác. Ước lượng cho $\hat f$ không nên thay đổi quá nhiều khi thay đổi các tập train. Nếu một phương pháp có variance cao thì sự thay đổi nhỏ trong tập dữ liệu huấn luyện cũng làm $\hat f$ thay đổi đáng kể. Các phương pháp có độ phức tạp <em>flexible</em> càng cao thì phương sai càng lớn.</p>
<p><em>Bias</em> là lỗi xảy ra khi xấp xỉ một mô hình phức tạp bằng một mô hình quá đơn giản. Ví dụ, khi ta ước lượng một mối quan hệ phi tuyến (như $Y = X^2$ chẳng hạn) bằng một hàm tuyến tính $\hat f$.</p>
<p>Khi độ phức tạp càng cao, bias càng nhỏ còn variance càng tăng, $MSE$ có thể sẽ giảm xuống. Đến một mốc nào đó việc tăng độ phức tạp có ảnh hưởng rất nhỏ đến Bias nhưng lại làm tăng đáng kể variance, khiến MSE tăng.
<img alt="Changes of Bias and variance" loading="lazy" src="https://uyennbu.github.io/img/2_12.png">
Hình trên cho thấy sự thay đổi của MSE test, bias, variance khi độ phức tạp tăng lên trong 3 trường hợp ở phần trên theo thứ tự tương ứng.
<img alt="Bias vs variance" loading="lazy" src="https://uyennbu.github.io/img/bias_vs_var.png"></p>
<h3 id="223-tiếp-cận-cho-bài-toán-phân-loại">2.2.3 Tiếp cận cho bài toán phân loại<a hidden class="anchor" aria-hidden="true" href="#223-tiếp-cận-cho-bài-toán-phân-loại">#</a></h3>
<p>Trong bài toán phân loại, tập dữ liệu huấn luyện của ta là $\{(x_1, y_1), ..., (x_n, y_n)\}$ trong đó $y_1, ..., y_n$ là các giá trị định tính. Cách phổ biến nhất để đánh giá độ chính xác của $\hat f$ là tỉ lệ lỗi huấn luyện <em>training error rate</em>:
</p>
$$\frac{1}{n}\sum_{i=1}^{n}I(y_i \neq \hat{y_i})$$<p>
trong đó $\hat {y_i}$ là nhãn phân loại dự đoán cho quan sát thứ $i$ sử dụng $\hat f$. $I(y_i \neq \hat{y_i})$ là <em>indicator variable</em>, bằng 1 nếu $y_i = \hat{y_i}$, bằng 0 nếu $y_i \neq \hat{y_i}$. Công thức trên dùng để tính <em>training error</em></p>
<p><em>Test error</em> được tính qua tập test:
</p>
$$\text{Ave}(I(y_i \neq \hat{y_i}))$$<p>Một mô hình phân loại tốt là mô hình có <em>test error</em> nhỏ nhất.</p>
<h4 id="bayes-classifier">Bayes classifier<a hidden class="anchor" aria-hidden="true" href="#bayes-classifier">#</a></h4>
<p><em>Bayes classifier</em> đưa ra dự đoán $y$ thuộc lớp $j$ dựa trên vector input $x_0$ mà xác suất:
</p>
$$\text{Pr}(Y=j|X=x_0)$$<p>
là lớn nhất. Đây là xác suất có điều kiện: xác suất $Y = j$ biết rằng $X = x_0$.</p>
<p>Bayes classifier có test error rate nhỏ nhất có thể, gọi là <em>Bayes error rate</em>, thường được tính qua công thức:
</p>
$$1 - E(\max_{j}\text{Pr}(Y=j|X))$$<p>
Bayes error rate giống với sai số không thể giảm được <em>irreducible error</em></p>
<h4 id="k-nearest-neighbor">K-nearest neighbor<a hidden class="anchor" aria-hidden="true" href="#k-nearest-neighbor">#</a></h4>
<p>Về mặt lý thuyết, ta luôn muốn sử dụng Bayes classifier. Tuy nhiên trên thực tế, ta không biết phân phối xác suất có điều kiện của Y theo X vì vậy tính Bayes classifier là không thể. Một trong những biện pháp thay thế là <em>K-nearest neighbors</em> (KNN) classifier.</p>
<p>Với một số thực K &gt; 0 cho trước và một điểm test $x_0$. KNN xác định tập $\mathcal{N}_0$ gồm K điểm trong tập train gần $x_0$ nhất sau đó tính:
</p>
$$\text{Pr}(Y=j|X=x_0)=\frac{1}{K}\sum_{i \in \mathcal{N}_0}I(y_i=j)$$<p>
KNN gán điểm test $x_0$ vào lớp có xác suất cao nhất tính theo công thức trên.</p>
<p>Minh họa về bài toán phân loại (hai lớp là cam và xanh). Đường màu đen là đường danh giới dự đoán của KNN, đường màu tím là của Bayes classifier (<em>Bayes decision boundary</em>).</p>
<p>Khi K=10 hai đường khá khớp với nhau:</p>
<p><img alt="k = 10" loading="lazy" src="https://uyennbu.github.io/img/2_15.jpg"></p>
<p>Đường K=1 quá phức tạp (flexible) trong khi đường K=100 quá đơn giản:</p>
<p><img alt="k = 100 vs k = 1" loading="lazy" src="https://uyennbu.github.io/img/2_16.jpg"></p>
<p>Sự thay đổi của train và test error rate khi K thay đổi (1/K tăng - tức K giảm ứng với việc tăng độ phức tạp của mô hình). Đường màu đen là Bayes error rate:</p>
<p><img alt="train/ test error rate changing as K changes" loading="lazy" src="https://uyennbu.github.io/img/2_17.jpg"></p>
<p>Có thể thấy K tốt nhất ứng với 1/K vào khoảng 0.1</p>
<hr>
<p>Bài viết của mình có thể còn nhiều thiếu sót, mình rất vui nếu được nhận góp ý từ bạn đọc để bài viết hoàn thiện và trở nên tốt hơn. Chúc bạn một ngày tốt lành ☘️</p>
<p>Email của mình : <a href="mailto:uyennguyen.nbu@gmail.com">uyennguyen.nbu@gmail.com</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://uyennbu.github.io/tags/statistics/">Statistics</a></li>
      <li><a href="https://uyennbu.github.io/tags/machine-learning/">Machine Learning</a></li>
    </ul>
<nav class="paginav">
  <a class="next" href="https://uyennbu.github.io/book/islp/ch2_1/">
    <span class="title">Next »</span>
    <br>
    <span>Chapter 2: Introduction to Statistical Learning (1)</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Chapter 2: Introduction to Statistical Learning (2) on x"
            href="https://x.com/intent/tweet/?text=Chapter%202%3a%20Introduction%20to%20Statistical%20Learning%20%282%29&amp;url=https%3a%2f%2fuyennbu.github.io%2fbook%2fislp%2fch2_2%2f&amp;hashtags=statistics%2cmachinelearning">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Chapter 2: Introduction to Statistical Learning (2) on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fuyennbu.github.io%2fbook%2fislp%2fch2_2%2f&amp;title=Chapter%202%3a%20Introduction%20to%20Statistical%20Learning%20%282%29&amp;summary=Chapter%202%3a%20Introduction%20to%20Statistical%20Learning%20%282%29&amp;source=https%3a%2f%2fuyennbu.github.io%2fbook%2fislp%2fch2_2%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Chapter 2: Introduction to Statistical Learning (2) on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fuyennbu.github.io%2fbook%2fislp%2fch2_2%2f&title=Chapter%202%3a%20Introduction%20to%20Statistical%20Learning%20%282%29">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Chapter 2: Introduction to Statistical Learning (2) on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fuyennbu.github.io%2fbook%2fislp%2fch2_2%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Chapter 2: Introduction to Statistical Learning (2) on whatsapp"
            href="https://api.whatsapp.com/send?text=Chapter%202%3a%20Introduction%20to%20Statistical%20Learning%20%282%29%20-%20https%3a%2f%2fuyennbu.github.io%2fbook%2fislp%2fch2_2%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Chapter 2: Introduction to Statistical Learning (2) on telegram"
            href="https://telegram.me/share/url?text=Chapter%202%3a%20Introduction%20to%20Statistical%20Learning%20%282%29&amp;url=https%3a%2f%2fuyennbu.github.io%2fbook%2fislp%2fch2_2%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Chapter 2: Introduction to Statistical Learning (2) on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Chapter%202%3a%20Introduction%20to%20Statistical%20Learning%20%282%29&u=https%3a%2f%2fuyennbu.github.io%2fbook%2fislp%2fch2_2%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="https://uyennbu.github.io/">Uyen&#39;s blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
