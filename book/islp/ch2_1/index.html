<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Chapter 2: Introduction to Statistical Learning (1) | Uyen&#39;s blog</title>
<meta name="keywords" content="statistics, machine learning">
<meta name="description" content="Bài viết nằm trong series ISLP, là series mình tóm tắt lại những gì mình đọc trong cuốn &ldquo;An Introduction to Statistical Learning with applications in Python&rdquo;.

Trang web của quyển sách: statlearning.com
Trang resources để tải file sách pdf, tải code và data:  resources
Bài giảng của tác giả được cung cấp bởi đại học Stanford: Statistical Learning with Python

2.1 Statistical learning là gì?
Ta bắt đầu bằng một bài toán:">
<meta name="author" content="">
<link rel="canonical" href="https://uyennbu.github.io/book/islp/ch2_1/">
<link crossorigin="anonymous" href="https://uyennbu.github.io/assets/css/stylesheet.da3211e5ef867bf2b75fd5a6515cfed7195c011e8ab735694e203810a827097b.css" integrity="sha256-2jIR5e&#43;Ge/K3X9WmUVz&#43;1xlcAR6KtzVpTiA4EKgnCXs=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://uyennbu.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://uyennbu.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://uyennbu.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://uyennbu.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://uyennbu.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://uyennbu.github.io/book/islp/ch2_1/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"></script>

<script>
  MathJax = {
    tex: {
      displayMath: [['\[', '\]'], ['$$', '$$']],  
      inlineMath: [['$', '$']]                  
    },
    loader:{
      load: ['ui/safe']
    },
  };
</script>
<meta property="og:url" content="https://uyennbu.github.io/book/islp/ch2_1/">
  <meta property="og:site_name" content="Uyen&#39;s blog">
  <meta property="og:title" content="Chapter 2: Introduction to Statistical Learning (1)">
  <meta property="og:description" content="Bài viết nằm trong series ISLP, là series mình tóm tắt lại những gì mình đọc trong cuốn “An Introduction to Statistical Learning with applications in Python”.
Trang web của quyển sách: statlearning.com Trang resources để tải file sách pdf, tải code và data: resources Bài giảng của tác giả được cung cấp bởi đại học Stanford: Statistical Learning with Python 2.1 Statistical learning là gì? Ta bắt đầu bằng một bài toán:">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="book">
    <meta property="article:published_time" content="2026-01-16T00:00:00+00:00">
    <meta property="article:modified_time" content="2026-01-16T00:00:00+00:00">
    <meta property="article:tag" content="Statistics">
    <meta property="article:tag" content="Machine Learning">
      <meta property="og:see_also" content="https://uyennbu.github.io/book/islp/ch2_2/">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 2: Introduction to Statistical Learning (1)">
<meta name="twitter:description" content="Bài viết nằm trong series ISLP, là series mình tóm tắt lại những gì mình đọc trong cuốn &ldquo;An Introduction to Statistical Learning with applications in Python&rdquo;.

Trang web của quyển sách: statlearning.com
Trang resources để tải file sách pdf, tải code và data:  resources
Bài giảng của tác giả được cung cấp bởi đại học Stanford: Statistical Learning with Python

2.1 Statistical learning là gì?
Ta bắt đầu bằng một bài toán:">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Books",
      "item": "https://uyennbu.github.io/book/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Chapter 2: Introduction to Statistical Learning (1)",
      "item": "https://uyennbu.github.io/book/islp/ch2_1/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Chapter 2: Introduction to Statistical Learning (1)",
  "name": "Chapter 2: Introduction to Statistical Learning (1)",
  "description": "Bài viết nằm trong series ISLP, là series mình tóm tắt lại những gì mình đọc trong cuốn \u0026ldquo;An Introduction to Statistical Learning with applications in Python\u0026rdquo;.\nTrang web của quyển sách: statlearning.com Trang resources để tải file sách pdf, tải code và data: resources Bài giảng của tác giả được cung cấp bởi đại học Stanford: Statistical Learning with Python 2.1 Statistical learning là gì? Ta bắt đầu bằng một bài toán:\n",
  "keywords": [
    "statistics", "machine learning"
  ],
  "articleBody": "Bài viết nằm trong series ISLP, là series mình tóm tắt lại những gì mình đọc trong cuốn “An Introduction to Statistical Learning with applications in Python”.\nTrang web của quyển sách: statlearning.com Trang resources để tải file sách pdf, tải code và data: resources Bài giảng của tác giả được cung cấp bởi đại học Stanford: Statistical Learning with Python 2.1 Statistical learning là gì? Ta bắt đầu bằng một bài toán:\nGiả sử chúng ta cần tìm hiểu mối liên kết giữa việc quảng cáo (advertising) và doanh số (sales) của một sản phẩm nào đó. Bộ dữ liệu Advertising bao gồm sales của sản phẩm đó trên 200 thị trường khác nhau, cùng với chi phí quảng cáo (advertising budgets) cho sản phẩm trong từng thị trường này cho 3 phương tiện truyền thông: TV, radio và newspaper. Nếu chúng ta tìm ra được mối quan hệ giữa chi phí quảng cáo cho từng phương tiện và doanh số chúng ta có thể điều chỉnh chi phí cho phù hợp để đạt được doanh thu cao hơn.\nTrong trường hợp này chi phí quảng cáo là input variables, được kí hiệu là $X$: $X_1, X_2, X_3$ lần lượt là chi phí quảng cáo bằng TV, radio và newspaper. input còn được gọi với các tên khác như predictors, independent variables (biến độc lập), features (đặc trưng) hoặc chỉ đơn giản là variables (biến) sales là một output variable, hay còn được gọi là response hoặc dependent variable (biến phụ thuộc), được kí hiệu là $Y$ Giả sử chúng ta quan sát được (thu được) dữ liệu $Y$ và $p$ biến khác nhau: $X_1, X_2, ..., X_p$. Ta giả sử rằng có một mối quan hệ nào đó giữa $Y$ và $X = (X_1, X_2, ..., X_p)$. Mối quan hệ này có thể viết khái quát bằng dạng: $$Y = f(X) + \\epsilon$$ Ở đây, $f$ là một hàm cố định nhưng chưa biết của $X_1, ..., X_p$ và $\\epsilon$ là lỗi ngẫu nhiên (random error term). $\\epsilon$ không phụ thuộc và $X$ và có kỳ vọng bằng 0. Hàm $f$ thể hiện thông tin có hệ thống mà $X$ cung cấp về $Y$.\nTóm lại, statistical learning nói đến một tập các cách tiếp cận để ước lượng hàm $f$.\n2.1.1 Mục đích của việc ước lượng $f$ Có hai mục đích chính cho việc ước lượng làm $f$: dự đoán (prediction) và suy luận (inference).\nDự đoán (prediction) Ta ước lượng hàm $f$ để dự đoán giá trị của $Y$ khi biết giá trị của $X$: $$\\hat Y = \\hat f(X)$$ ở đây $\\hat f$ và $\\hat Y$ lần lượt là các giá trị ước lượng của $f$ và $Y$.\nTrong trường hợp này, ta chỉ quan tâm đến độ chính xác của giá trị ước lượng cho $Y$ mà không quan tâm đến hàm $f$ thật sự là gì, vì thế hàm $f$ thường được coi là một black box.\nError: Độ chính xác của $\\hat Y$ như là một dự đoán cho $Y$ phụ thuộc vào hai yếu tố: reducible error (lỗi có thể giảm được) và irreducible error (lỗi không thể giảm được).\nreducible error: Nhìn chung $\\hat f$ không phải là ước lượng hoàn hảo cho $f$, tạo ra reducible error. Lỗi này giảm được vì ta có khả năng tìm được một hàm khác tốt hơn. irreduciable error: Dù cho ta có tìm được chính xác hàm $f$ thì ta không thể dự đoán chính xác $Y$ do $Y$ cũng là một hàm phụ thuộc vào $\\epsilon$, một đại lượng không phụ thuộc vào $X$ ($Y = f(X) + \\epsilon$). Đại lượng $\\epsilon$ đại diện cho những biến khác mà có thể cũng hữu ích nhưng ta không đo lường nó và không đưa nó vào hàm $f$ (unmeasured variables) hoặc những ngẫu nhiên khác (unmeasured variation). $$ \\begin{split} E(Y - \\hat Y)^2 \u0026 = E[f(X) + \\epsilon - \\hat f(X)] \\\\ \u0026 = [f(X) - \\hat f(X)]^2 + Var(\\epsilon)\\\\ \u0026\\quad \\text(Reducible) \\quad \\quad(Irreducible) \\end{split} $$ trong đó, $E(Y - \\hat Y)^2$ là giá trị trung bình hay kì vọng (expected value) của bình phương chênh lệch giữa giá trị dự đoán và giá trị thực của $Y$ và Var($\\epsilon$) là phương sai (variance) ứng với error term $\\epsilon$ Suy luận (inference) Ta muốn hiểu về mối liên hết giữa $Y$ và $X$. Trong trường hợp này, mục tiêu của ta không phải là dự đoán chính xác $Y$ mà là tìm ra dạng giống nhất đối với $f$. Khi này, ta không thể coi $\\hat f$ là một black box được nữa.\nTrong trường hợp này, ta có thể muốn trả lời một số câu hỏi:\nBiến nào có ảnh hưởng lớn nhất đến output? Mối quan hệ giữa output và từng biến đầu vào là gì? Mối quan hệ giữa output và input là tuyến tính hay là một mối quan hệ phức tạp hơn? 2.1.2 Làm thế nào để ước lượng $f$? Khái niệm:\nmodel (mô hình): là hàm số $f$ hay $\\hat f$ training data (dữ liệu huấn luyện): tập dữ liệu ta thu được, dùng để ước lượng hàm $f$ Kí hiệu:\n$n$: số điểm dữ liệu thu được (số observations) $p$: số biến $x_{ij}$ là giá trị của biến thứ $j$ của điểm thứ $i$ (observation $i$) ($i = 1, 2, 3, ..., n$ và $j = 1, 2, ..., p$) $y_i$ phản hồi của observation $i$ Dữ liệu huấn luyện có thể biểu diễn dưới dạng $\\{(x_1, y_1), ..., (x_n, y_n)\\}$ trong đó $x_i = (x_{i1}, x_{i2}, ..., x_{ip})^T$ Có hai cách chính để ước lượng f: tham số (parametric methods) và phi tham số (non-paramatric methods)\nParametric methods Là cách tiếp cận dựa trên mô hình (hàm), bao gồm 2 bước\nĐưa ra giả định về dạng của hàm $f$. Ví dụ ta có thể giả định rằng $f$ là tuyến tính: $$f(X) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p$$ Dùng training data để fit hoặc train model, tức là ước lượng các tham số $\\beta_0, ..., \\beta_p$ sao cho: $$Y \\approx \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p$$ Các tiếp cận này đơn giản hóa việc ước lượng hàm $f$ bằng cách đưa nó về ước lượng các tham số. Tuy nhiên, nhược điểm là mô hình được chọn ban đầu có thể rất khác với hàm $f$ thực tế. Để tránh điều này ta có thể dùng các mô hình phức tạp (flexible models), giúp fit nhiều dạng hàm có thể xảy ra hơn. Nhưng mô hình phức tạp hơn thì có nguy cơ xảy ra overfitting cao hơn (overfitting xảy ra khi mô hình học theo cả lỗi hay noise trong dữ liệu)\nNon-parametric methods Các phương pháp phi tham số không đặt giả định rõ ràng về dạng của $f$, mà tìm một ước lượng mà gần với các điểm dữ liệu nhất có thể mà không quá gồ ghề (rough)\n2.1.3 Sự đánh đổi giữa độ chính xác và khả năng giải thích của mô hình Mô hình càng phức tạp (càng flexible) thì khả năng giải thích càng kém (hãy nghĩ đến black box)\n2.1.4 Có giám sát vs không giám sát Phần lớn các vấn đề có thể chia làm 2 loại: học có giám sát (supervised learning) và học không giám sát (unsupervised learning)\nHọc có giám sát: Với mỗi predictor $x_i$ có phản hồi $y_i$ tương ứng, ta muốn huấn luyện mô hình để đưa ra đúng dự đoán về $y$ dựa trên $x$ hoặc hiểu rõ hơn mối quan hệ giữa $x$ và $y$ Chia thành bài toán hồi quy (regression) hoặc bài toán phân loại (classification) Học không giám sát: $x_i$ không có giá trị $y_i$ tương ứng, ta muốn tìm hiểu về mối quan hệ giữa các giá trị hay các observations với nhau. Tiêu biểu như bài toán phân cụm (clustering) Học bán giám sát (semi-supervised learning): khi một phần dữ liệu có nhãn, một phần khác thì không. 2.1.5 Hồi quy vs phân loại Các giá trị có thể là định lượng (quantitative) (các giá trị số như tuổi, chiều cao, …) hoặc định tính (quanlitative, hay categorical) (các giá trị phân loại như tình trạng hôn nhân, nhóm máu, …)\nHồi quy (regression): output $y$ là giá trị định lượng\nPhân loại (classification): output $y$ là giá trị định tính\nBài viết của mình có thể còn nhiều thiếu sót, mình rất vui nếu được nhận góp ý từ bạn đọc để bài viết hoàn thiện và trở nên tốt hơn. Chúc bạn một ngày tốt lành ☘️\nEmail của mình : uyennguyen.nbu@gmail.com\n",
  "wordCount" : "1450",
  "inLanguage": "en",
  "datePublished": "2026-01-16T00:00:00Z",
  "dateModified": "2026-01-16T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://uyennbu.github.io/book/islp/ch2_1/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Uyen's blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://uyennbu.github.io/favicon.ico"
    }
  }
}
</script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://uyennbu.github.io/" accesskey="h" title="Uyen&#39;s blog (Alt + H)">Uyen&#39;s blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://uyennbu.github.io/series/" title="Series">
                    <span>Series</span>
                </a>
            </li>
            <li>
                <a href="https://uyennbu.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://uyennbu.github.io/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://uyennbu.github.io/about/" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://uyennbu.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://uyennbu.github.io/book/">Books</a></div>
    <h1 class="post-title entry-hint-parent">
      Chapter 2: Introduction to Statistical Learning (1)
    </h1>
    <div class="post-meta"><span title='2026-01-16 00:00:00 +0000 UTC'>January 16, 2026</span>&nbsp;·&nbsp;<span>7 min</span>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#21-statistical-learning-l%c3%a0-g%c3%ac" aria-label="2.1 Statistical learning là gì?">2.1 Statistical learning là gì?</a><ul>
                        
                <li>
                    <a href="#211-m%e1%bb%a5c-%c4%91%c3%adch-c%e1%bb%a7a-vi%e1%bb%87c-%c6%b0%e1%bb%9bc-l%c6%b0%e1%bb%a3ng" aria-label="2.1.1 Mục đích của việc ước lượng $f$">2.1.1 Mục đích của việc ước lượng $f$</a><ul>
                        
                <li>
                    <a href="#d%e1%bb%b1-%c4%91o%c3%a1n-prediction" aria-label="Dự đoán (prediction)">Dự đoán (prediction)</a></li>
                <li>
                    <a href="#suy-lu%e1%ba%adn-inference" aria-label="Suy luận (inference)">Suy luận (inference)</a></li></ul>
                </li>
                <li>
                    <a href="#212-l%c3%a0m-th%e1%ba%bf-n%c3%a0o-%c4%91%e1%bb%83-%c6%b0%e1%bb%9bc-l%c6%b0%e1%bb%a3ng-" aria-label="2.1.2 Làm thế nào để ước lượng $f$?">2.1.2 Làm thế nào để ước lượng $f$?</a><ul>
                        
                <li>
                    <a href="#parametric-methods" aria-label="Parametric methods">Parametric methods</a></li>
                <li>
                    <a href="#non-parametric-methods" aria-label="Non-parametric methods">Non-parametric methods</a></li></ul>
                </li>
                <li>
                    <a href="#213-s%e1%bb%b1-%c4%91%c3%a1nh-%c4%91%e1%bb%95i-gi%e1%bb%afa-%c4%91%e1%bb%99-ch%c3%adnh-x%c3%a1c-v%c3%a0-kh%e1%ba%a3-n%c4%83ng-gi%e1%ba%a3i-th%c3%adch-c%e1%bb%a7a-m%c3%b4-h%c3%acnh" aria-label="2.1.3 Sự đánh đổi giữa độ chính xác và khả năng giải thích của mô hình">2.1.3 Sự đánh đổi giữa độ chính xác và khả năng giải thích của mô hình</a></li>
                <li>
                    <a href="#214-c%c3%b3-gi%c3%a1m-s%c3%a1t-vs-kh%c3%b4ng-gi%c3%a1m-s%c3%a1t" aria-label="2.1.4 Có giám sát vs không giám sát">2.1.4 Có giám sát vs không giám sát</a></li>
                <li>
                    <a href="#215-h%e1%bb%93i-quy-vs-ph%c3%a2n-lo%e1%ba%a1i" aria-label="2.1.5 Hồi quy vs phân loại">2.1.5 Hồi quy vs phân loại</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>Bài viết nằm trong series ISLP, là series mình tóm tắt lại những gì mình đọc trong cuốn &ldquo;An Introduction to Statistical Learning with applications in Python&rdquo;.</p>
<ul>
<li>Trang web của quyển sách: <a href="https://www.statlearning.com/">statlearning.com</a></li>
<li>Trang resources để tải file sách pdf, tải code và data:  <a href="https://www.statlearning.com/resources-python">resources</a></li>
<li>Bài giảng của tác giả được cung cấp bởi đại học Stanford: <a href="https://youtube.com/playlist?list=PLoROMvodv4rPP6braWoRt5UCXYZ71GZIQ&amp;si=wvVcrbaPvFn4x9wb">Statistical Learning with Python</a></li>
</ul>
<h2 id="21-statistical-learning-là-gì">2.1 Statistical learning là gì?<a hidden class="anchor" aria-hidden="true" href="#21-statistical-learning-là-gì">#</a></h2>
<p>Ta bắt đầu bằng một bài toán:</p>
<p>Giả sử chúng ta cần tìm hiểu mối liên kết giữa việc quảng cáo (advertising) và doanh số (sales) của một sản phẩm nào đó. Bộ dữ liệu <code>Advertising</code> bao gồm <code>sales</code> của sản phẩm đó trên 200 thị trường khác nhau, cùng với chi phí quảng cáo (advertising budgets) cho sản phẩm trong từng thị trường này cho 3 phương tiện truyền thông: <code>TV</code>, <code>radio</code> và <code>newspaper</code>. Nếu chúng ta tìm ra được mối quan hệ giữa chi phí quảng cáo cho từng phương tiện và doanh số chúng ta có thể điều chỉnh chi phí cho phù hợp để đạt được doanh thu cao hơn.</p>
<ul>
<li>Trong trường hợp này chi phí quảng cáo là <em>input variables</em>, được kí hiệu là $X$: $X_1, X_2, X_3$ lần lượt là chi phí quảng cáo bằng <code>TV</code>, <code>radio</code> và <code>newspaper</code>. <em>input</em> còn được gọi với các tên khác như <em>predictors</em>, <em>independent variables</em> (biến độc lập), <em>features</em> (đặc trưng) hoặc chỉ đơn giản là <em>variables</em> (biến)</li>
<li><code>sales</code> là một <em>output variable</em>, hay còn được gọi là <em>response</em> hoặc <em>dependent variable</em> (biến phụ thuộc), được kí hiệu là $Y$</li>
</ul>
<p>Giả sử chúng ta quan sát được (thu được) dữ liệu $Y$ và $p$ biến khác nhau: $X_1, X_2, ..., X_p$. Ta giả sử rằng có một mối quan hệ nào đó giữa $Y$ và $X = (X_1, X_2, ..., X_p)$. Mối quan hệ này có thể viết khái quát bằng dạng:
</p>
$$Y = f(X) + \epsilon$$<p>
Ở đây, $f$ là một hàm cố định nhưng chưa biết của $X_1, ..., X_p$ và $\epsilon$ là lỗi ngẫu nhiên (<em>random error term</em>). $\epsilon$ không phụ thuộc và $X$ và có kỳ vọng bằng 0. Hàm $f$ thể hiện thông tin có hệ thống mà $X$ cung cấp về $Y$.</p>
<p>Tóm lại, statistical learning nói đến một tập các cách tiếp cận để ước lượng hàm $f$.</p>
<h3 id="211-mục-đích-của-việc-ước-lượng">2.1.1 Mục đích của việc ước lượng $f$<a hidden class="anchor" aria-hidden="true" href="#211-mục-đích-của-việc-ước-lượng">#</a></h3>
<p>Có hai mục đích chính cho việc ước lượng làm $f$: dự đoán (prediction) và suy luận (inference).</p>
<h4 id="dự-đoán-prediction">Dự đoán (prediction)<a hidden class="anchor" aria-hidden="true" href="#dự-đoán-prediction">#</a></h4>
<p>Ta ước lượng hàm $f$ để dự đoán giá trị của $Y$ khi biết giá trị của $X$:
</p>
$$\hat Y = \hat f(X)$$<p>
ở đây $\hat f$ và $\hat Y$ lần lượt là các giá trị ước lượng của $f$ và $Y$.</p>
<p>Trong trường hợp này, ta chỉ quan tâm đến độ chính xác của giá trị ước lượng cho $Y$ mà không quan tâm đến hàm $f$ thật sự là gì, vì thế hàm $f$ thường được coi là một <em>black box</em>.</p>
<p><strong>Error:</strong>
Độ chính xác của $\hat Y$ như là một dự đoán cho $Y$ phụ thuộc vào hai yếu tố: <em>reducible error</em> (lỗi có thể giảm được) và <em>irreducible error</em> (lỗi không thể giảm được).</p>
<ul>
<li><em>reducible error</em>: Nhìn chung $\hat f$ không phải là ước lượng hoàn hảo cho $f$, tạo ra <em>reducible error</em>. Lỗi này giảm được vì ta có khả năng tìm được một hàm khác tốt hơn.</li>
<li><em>irreduciable error</em>: Dù cho ta có tìm được chính xác hàm $f$ thì ta không thể dự đoán chính xác $Y$ do $Y$ cũng là một hàm phụ thuộc vào $\epsilon$, một đại lượng không phụ thuộc vào $X$ ($Y = f(X) + \epsilon$). Đại lượng $\epsilon$ đại diện cho những biến khác mà có thể cũng hữu ích nhưng ta không đo lường nó và không đưa nó vào hàm $f$ (unmeasured variables) hoặc những ngẫu nhiên khác (unmeasured variation).
$$
\begin{split}
  E(Y - \hat Y)^2 & = E[f(X) + \epsilon - \hat f(X)] \\
                  & = [f(X) - \hat f(X)]^2 + Var(\epsilon)\\
                  &\quad \text(Reducible) \quad \quad(Irreducible)
\end{split}
$$
trong đó, $E(Y - \hat Y)^2$ là giá trị trung bình hay kì vọng (<em>expected value</em>) của bình phương chênh lệch giữa giá trị dự đoán và giá trị thực của $Y$ và Var($\epsilon$) là phương sai (<em>variance</em>) ứng với error term $\epsilon$</li>
</ul>
<h4 id="suy-luận-inference">Suy luận (inference)<a hidden class="anchor" aria-hidden="true" href="#suy-luận-inference">#</a></h4>
<p>Ta muốn hiểu về mối liên hết giữa $Y$ và $X$. Trong trường hợp này, mục tiêu của ta không phải là dự đoán chính xác $Y$ mà là tìm ra dạng giống nhất đối với $f$. Khi này, ta không thể coi $\hat f$ là một black box được nữa.</p>
<p>Trong trường hợp này, ta có thể muốn trả lời một số câu hỏi:</p>
<ul>
<li>Biến nào có ảnh hưởng lớn nhất đến output?</li>
<li>Mối quan hệ giữa output và từng biến đầu vào là gì?</li>
<li>Mối quan hệ giữa output và input là tuyến tính hay là một mối quan hệ phức tạp hơn?</li>
</ul>
<h3 id="212-làm-thế-nào-để-ước-lượng-">2.1.2 Làm thế nào để ước lượng $f$?<a hidden class="anchor" aria-hidden="true" href="#212-làm-thế-nào-để-ước-lượng-">#</a></h3>
<p>Khái niệm:</p>
<ul>
<li><em>model</em> (mô hình): là hàm số $f$ hay $\hat f$</li>
<li><em>training data</em> (dữ liệu huấn luyện): tập dữ liệu ta thu được, dùng để ước lượng hàm $f$</li>
</ul>
<p>Kí hiệu:</p>
<ul>
<li>$n$: số điểm dữ liệu thu được (số observations)</li>
<li>$p$: số biến</li>
<li>$x_{ij}$ là giá trị của biến thứ $j$ của điểm thứ $i$ (observation $i$) ($i = 1, 2, 3, ..., n$ và $j = 1, 2, ..., p$)</li>
<li>$y_i$ phản hồi của observation $i$
Dữ liệu huấn luyện có thể biểu diễn dưới dạng $\{(x_1, y_1), ..., (x_n, y_n)\}$ trong đó $x_i = (x_{i1}, x_{i2}, ..., x_{ip})^T$</li>
</ul>
<p>Có hai cách chính để ước lượng f: tham số (<em>parametric methods</em>) và phi tham số (<em>non-paramatric methods</em>)</p>
<h4 id="parametric-methods">Parametric methods<a hidden class="anchor" aria-hidden="true" href="#parametric-methods">#</a></h4>
<p>Là cách tiếp cận dựa trên mô hình (hàm), bao gồm 2 bước</p>
<ol>
<li>Đưa ra giả định về dạng của hàm $f$. Ví dụ ta có thể giả định rằng $f$ là tuyến tính:
$$f(X) = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p$$</li>
<li>Dùng training data để <em>fit</em> hoặc <em>train model</em>, tức là ước lượng các tham số $\beta_0, ..., \beta_p$ sao cho:
$$Y \approx \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p$$</li>
</ol>
<p>Các tiếp cận này đơn giản hóa việc ước lượng hàm $f$ bằng cách đưa nó về ước lượng các tham số. Tuy nhiên, nhược điểm là mô hình được chọn ban đầu có thể rất khác với hàm $f$ thực tế. Để tránh điều này ta có thể dùng các mô hình phức tạp (<em>flexible models</em>), giúp fit nhiều dạng hàm có thể xảy ra hơn. Nhưng mô hình phức tạp hơn thì có nguy cơ xảy ra <em>overfitting</em> cao hơn (<em>overfitting</em> xảy ra khi mô hình học theo cả lỗi hay <em>noise</em> trong dữ liệu)</p>
<h4 id="non-parametric-methods">Non-parametric methods<a hidden class="anchor" aria-hidden="true" href="#non-parametric-methods">#</a></h4>
<p>Các phương pháp phi tham số không đặt giả định rõ ràng về dạng của $f$, mà tìm một ước lượng mà gần với các điểm dữ liệu nhất có thể mà không quá gồ ghề (<em>rough</em>)</p>
<h3 id="213-sự-đánh-đổi-giữa-độ-chính-xác-và-khả-năng-giải-thích-của-mô-hình">2.1.3 Sự đánh đổi giữa độ chính xác và khả năng giải thích của mô hình<a hidden class="anchor" aria-hidden="true" href="#213-sự-đánh-đổi-giữa-độ-chính-xác-và-khả-năng-giải-thích-của-mô-hình">#</a></h3>
<p>Mô hình càng phức tạp (càng <em>flexible</em>) thì khả năng giải thích càng kém (hãy nghĩ đến black box)</p>
<p><img alt="A representation of the tradeoff between flexibility and interpretability" loading="lazy" src="https://www.stat.cmu.edu/~pfreeman/flexibility.png"></p>
<h3 id="214-có-giám-sát-vs-không-giám-sát">2.1.4 Có giám sát vs không giám sát<a hidden class="anchor" aria-hidden="true" href="#214-có-giám-sát-vs-không-giám-sát">#</a></h3>
<p>Phần lớn các vấn đề có thể chia làm 2 loại: học có giám sát (<em>supervised learning</em>) và học không giám sát (<em>unsupervised learning</em>)</p>
<ul>
<li>Học có giám sát: Với mỗi predictor $x_i$ có phản hồi $y_i$ tương ứng, ta muốn huấn luyện mô hình để đưa ra đúng dự đoán về $y$ dựa trên $x$ hoặc hiểu rõ hơn mối quan hệ giữa $x$ và $y$
<ul>
<li>Chia thành bài toán hồi quy (<em>regression</em>) hoặc bài toán phân loại (<em>classification</em>)</li>
</ul>
</li>
<li>Học không giám sát: $x_i$ không có giá trị $y_i$ tương ứng, ta muốn tìm hiểu về mối quan hệ giữa các giá trị hay các observations với nhau.
<ul>
<li>Tiêu biểu như bài toán phân cụm (<em>clustering</em>)</li>
</ul>
</li>
<li>Học bán giám sát (<em>semi-supervised learning</em>): khi một phần dữ liệu có nhãn, một phần khác thì không.</li>
</ul>
<h3 id="215-hồi-quy-vs-phân-loại">2.1.5 Hồi quy vs phân loại<a hidden class="anchor" aria-hidden="true" href="#215-hồi-quy-vs-phân-loại">#</a></h3>
<p>Các giá trị có thể là định lượng (<em>quantitative</em>) (các giá trị số như tuổi, chiều cao, &hellip;) hoặc định tính (<em>quanlitative</em>, hay <em>categorical</em>) (các giá trị phân loại như tình trạng hôn nhân, nhóm máu, &hellip;)</p>
<p><strong>Hồi quy</strong> (<em>regression</em>): output $y$ là giá trị định lượng</p>
<p><strong>Phân loại</strong> (<em>classification</em>): output $y$ là giá trị định tính</p>
<hr>
<p>Bài viết của mình có thể còn nhiều thiếu sót, mình rất vui nếu được nhận góp ý từ bạn đọc để bài viết hoàn thiện và trở nên tốt hơn. Chúc bạn một ngày tốt lành ☘️</p>
<p>Email của mình : <a href="mailto:uyennguyen.nbu@gmail.com">uyennguyen.nbu@gmail.com</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://uyennbu.github.io/tags/statistics/">Statistics</a></li>
      <li><a href="https://uyennbu.github.io/tags/machine-learning/">Machine Learning</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://uyennbu.github.io/book/islp/ch2_2/">
    <span class="title">« Prev</span>
    <br>
    <span>Chapter 2: Introduction to Statistical Learning (2)</span>
  </a>
  <a class="next" href="https://uyennbu.github.io/book/studyinhus/ratingsubjects/">
    <span class="title">Next »</span>
    <br>
    <span>Các môn mình học tại HUS</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Chapter 2: Introduction to Statistical Learning (1) on x"
            href="https://x.com/intent/tweet/?text=Chapter%202%3a%20Introduction%20to%20Statistical%20Learning%20%281%29&amp;url=https%3a%2f%2fuyennbu.github.io%2fbook%2fislp%2fch2_1%2f&amp;hashtags=statistics%2cmachinelearning">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Chapter 2: Introduction to Statistical Learning (1) on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fuyennbu.github.io%2fbook%2fislp%2fch2_1%2f&amp;title=Chapter%202%3a%20Introduction%20to%20Statistical%20Learning%20%281%29&amp;summary=Chapter%202%3a%20Introduction%20to%20Statistical%20Learning%20%281%29&amp;source=https%3a%2f%2fuyennbu.github.io%2fbook%2fislp%2fch2_1%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Chapter 2: Introduction to Statistical Learning (1) on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fuyennbu.github.io%2fbook%2fislp%2fch2_1%2f&title=Chapter%202%3a%20Introduction%20to%20Statistical%20Learning%20%281%29">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Chapter 2: Introduction to Statistical Learning (1) on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fuyennbu.github.io%2fbook%2fislp%2fch2_1%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Chapter 2: Introduction to Statistical Learning (1) on whatsapp"
            href="https://api.whatsapp.com/send?text=Chapter%202%3a%20Introduction%20to%20Statistical%20Learning%20%281%29%20-%20https%3a%2f%2fuyennbu.github.io%2fbook%2fislp%2fch2_1%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Chapter 2: Introduction to Statistical Learning (1) on telegram"
            href="https://telegram.me/share/url?text=Chapter%202%3a%20Introduction%20to%20Statistical%20Learning%20%281%29&amp;url=https%3a%2f%2fuyennbu.github.io%2fbook%2fislp%2fch2_1%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Chapter 2: Introduction to Statistical Learning (1) on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Chapter%202%3a%20Introduction%20to%20Statistical%20Learning%20%281%29&u=https%3a%2f%2fuyennbu.github.io%2fbook%2fislp%2fch2_1%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="https://uyennbu.github.io/">Uyen&#39;s blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
