<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Chapter 2: Introduction to Statistical Learning (2) | Uyen&#39;s blog</title>
<meta name="keywords" content="statistics">
<meta name="description" content="Bài viết nằm trong series ISLP, là series mình tóm tắt lại những gì mình đọc trong cuốn &ldquo;An Introduction to Statistical Learning with applications in Python&rdquo;.

Trang web của quyển sách: statlearning.com
Trang resources để tải file sách pdf, tải code và data:  resources
Bài giảng của tác giả được cung cấp bởi đại học Stanford: Statistical Learning with Python

2.2 Đánh giá sự chính xác
Chúng ta cần nhiều phương pháp thay vì một cách duy nhất vì không có model nào là phù hợp với tất cả các tập dữ liệu. Với một tập dữ liệu nhất định có thể phù hợp với phương pháp này, nhưng một tập dữ liệu tương tự khác lại biểu hiện tốt hơn với phương pháp khác. Chọn cách tiếp cận đúng có lẽ là một trong những nhiệm vụ quan trọng nhất trên thực tế.">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/book/islp/ch2_2/">
<link crossorigin="anonymous" href="http://localhost:1313/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css" integrity="sha256-NDzEgLn/yPBMy&#43;XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/book/islp/ch2_2/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"></script>

<script>
  MathJax = {
    tex: {
      displayMath: [['\[', '\]'], ['$$', '$$']],  
      inlineMath: [['$', '$']]                  
    },
    loader:{
      load: ['ui/safe']
    },
  };
</script>

</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Uyen&#39;s blog (Alt + H)">Uyen&#39;s blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/series/" title="Series">
                    <span>Series</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/about/" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/book/">Books</a></div>
    <h1 class="post-title entry-hint-parent">
      Chapter 2: Introduction to Statistical Learning (2)
    </h1>
    <div class="post-meta"><span title='2026-01-17 00:00:00 +0000 UTC'>January 17, 2026</span>&nbsp;·&nbsp;<span>6 min</span>

</div>
  </header> 
  <div class="post-content"><p>Bài viết nằm trong series ISLP, là series mình tóm tắt lại những gì mình đọc trong cuốn &ldquo;An Introduction to Statistical Learning with applications in Python&rdquo;.</p>
<ul>
<li>Trang web của quyển sách: <a href="https://www.statlearning.com/">statlearning.com</a></li>
<li>Trang resources để tải file sách pdf, tải code và data:  <a href="https://www.statlearning.com/resources-python">resources</a></li>
<li>Bài giảng của tác giả được cung cấp bởi đại học Stanford: <a href="https://youtube.com/playlist?list=PLoROMvodv4rPP6braWoRt5UCXYZ71GZIQ&amp;si=wvVcrbaPvFn4x9wb">Statistical Learning with Python</a></li>
</ul>
<h2 id="22-đánh-giá-sự-chính-xác">2.2 Đánh giá sự chính xác<a hidden class="anchor" aria-hidden="true" href="#22-đánh-giá-sự-chính-xác">#</a></h2>
<p>Chúng ta cần nhiều phương pháp thay vì một cách duy nhất vì không có model nào là phù hợp với tất cả các tập dữ liệu. Với một tập dữ liệu nhất định có thể phù hợp với phương pháp này, nhưng một tập dữ liệu tương tự khác lại biểu hiện tốt hơn với phương pháp khác. Chọn cách tiếp cận đúng có lẽ là một trong những nhiệm vụ quan trọng nhất trên thực tế.</p>
<h3 id="221-đo-lường-độ-fit">2.2.1 Đo lường độ fit<a hidden class="anchor" aria-hidden="true" href="#221-đo-lường-độ-fit">#</a></h3>
<p>Trong các bài toán hồi quy, để đo lường xem sự dự đoán của model thực sự gần với dữ liệu thực tế như thế nào, phương pháp thường được dùng nhiều nhất là bình phương tối thiểu (<em>mean squared error</em>) (MSE):
</p>
$$MSE = \frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{f}(x_i))^2$$<p>
Trong đó $\hat f(x_i)$ là dự đoán mà $\hat f$ đưa ra ứng với quan sát thứ i. Dự đoán càng gần với thực tế thì MSE càng nhỏ và ngược lại.</p>
<p>MSE được tính theo công thức trên là tính dựa trên tập train, gọi là <em>training MSE</em>. Tuy nhiên, mục tiêu cuối cùng của ta không phải là làm cho <em>training MSE</em> nhỏ nhất, mà là làm cho <em>test MSE</em> nhỏ nhất. Tức là độ chính xác là cao nhất khi chúng ta sử dụng model để dự đoán giá trị trong tập test, những giá trị mà không có trong tập huấn luyện ban đầu. Nếu ta có một lượng lớn các quan sát trong tập test, ta có thể tính:
</p>
$$\text{Ave}(y_0 + \hat f(x_0))^2$$<p>
sai số trung bình bình phương dự đoán cho những điểm test $(x_0, y_0)$ và chọn model cho ra giá trị này nhỏ nhất.</p>
<p>Trên thực tế, ta chỉ có một tập dữ liệu duy nhất. Thay vì dùng toàn bộ để train mô hình, ta chia dữ liệu thành các tập train và test khác nhau. Có nhiều phương pháp để làm điều này, sẽ được nhắc đến ở phần sau. Một ví dụ là <em>cross validation</em>.</p>
<p>Overfitting là khi mô hình quá khớp với dữ liệu train, do đó không còn đúng với mối quan hệ thực sự giữa $X$ và $Y$. Lúc này, một mô hình đơn giản hơn lại thể hiện tốt hơn một mô hình quá phức tạp.</p>
<p>Trong hình các sau hàm $f$ thực tế là đường màu đen, đường màu đen là đường hồi quy tuyến tính, hai đường màu xanh là $\hat f$ với độ phức tạp cao hơn.
<img alt="2.9" loading="lazy" src="http://localhost:1313/img/2_9.jpg">
Ở hình trên, có thể thấy đường hồi quy quá đơn giản so với $f$ thực tế. Trong khi ở hình thứ 2 này, mô hình hồi quy tuyến tính lại làm rất tốt so với các mô hình phức tạp hơn:
<img alt="2.10" loading="lazy" src="http://localhost:1313/img/2_10.jpg">
Một ví dụ khác:
<img alt="2.11" loading="lazy" src="http://localhost:1313/img/2_11.jpg"></p>
<h3 id="222-sự-đánh-đổi-giữa-bias-và-variance">2.2.2 Sự đánh đổi giữa bias và variance<a hidden class="anchor" aria-hidden="true" href="#222-sự-đánh-đổi-giữa-bias-và-variance">#</a></h3>
<p>Kỳ vọng của test MSE (<em>expected test MSE</em>) tại điểm $x_0$:
</p>
$$E(y_0 - \hat f(x_0))^2 = \text{Var}(\hat f(x_0)) + [\text{Bias}(\hat f(x_0))]^2 + \text{Var}(\epsilon)$$<p>
Để làm giảm </p>
$$E(y_0 - \hat f(x_0))^2$$<p> thì ta cần chọn phương pháp mà có phương sai thấp (<em>low variance</em>) và bias thấp (<em>low bias</em>).</p>
<p><em>Variance</em> là sự thay đổi của $\hat f$ nếu ta ước lượng nó bằng một tập dữ liệu training khác. Ước lượng cho $\hat f$ không nên thay đổi quá nhiều khi thay đổi các tập train. Nếu một phương pháp có variance cao thì sự thay đổi nhỏ trong tập dữ liệu huấn luyện cũng làm $\hat f$ thay đổi đáng kể. Các phương pháp có độ phức tạp <em>flexible</em> càng cao thì phương sai càng lớn.</p>
<p><em>Bias</em> là lỗi xảy ra khi xấp xỉ một mô hình phức tạp bằng một mô hình quá đơn giản. Ví dụ, khi ta ước lượng một mối quan hệ phi tuyến (như $Y = X^2$ chẳng hạn) bằng một hàm tuyến tính $\hat f$.</p>
<p>Khi độ phức tạp càng cao, bias càng nhỏ còn variance càng tăng, $MSE$ có thể sẽ giảm xuống. Đến một mốc nào đó việc tăng độ phức tạp có ảnh hưởng rất nhỏ đến Bias nhưng lại làm tăng đáng kể variance, khiến MSE tăng.
<img alt="Changes of Bias and variance" loading="lazy" src="http://localhost:1313/img/2_12.png">
Hình trên cho thấy sự thay đổi của MSE test, bias, variance khi độ phức tạp tăng lên trong 3 trường hợp ở phần trên theo thứ tự tương ứng.
<img alt="Bias vs variance" loading="lazy" src="http://localhost:1313/img/bias_vs_var.png"></p>
<h3 id="223-tiếp-cận-cho-bài-toán-phân-loại">2.2.3 Tiếp cận cho bài toán phân loại<a hidden class="anchor" aria-hidden="true" href="#223-tiếp-cận-cho-bài-toán-phân-loại">#</a></h3>
<p>Trong bài toán phân loại, tập dữ liệu huấn luyện của ta là $\{(x_1, y_1), ..., (x_n, y_n)\}$ trong đó $y_1, ..., y_n$ là các giá trị định tính. Cách phổ biến nhất để đánh giá độ chính xác của $\hat f$ là tỉ lệ lỗi huấn luyện <em>training error rate</em>:
</p>
$$\frac{1}{n}\sum_{i=1}^{n}I(y_i \neq \hat{y_i})$$<p>
trong đó $\hat {y_i}$ là nhãn phân loại dự đoán cho quan sát thứ $i$ sử dụng $\hat f$. $I(y_i \neq \hat{y_i})$ là <em>indicator variable</em>, bằng 1 nếu $y_i = \hat{y_i}$, bằng 0 nếu $y_i \neq \hat{y_i}$. Công thức trên dùng để tính <em>training error</em></p>
<p><em>Test error</em> được tính qua tập test:
</p>
$$\text{Ave}(I(y_i \neq \hat{y_i}))$$<p>Một mô hình phân loại tốt là mô hình có <em>test error</em> nhỏ nhất.</p>
<h4 id="bayes-classifier">Bayes classifier<a hidden class="anchor" aria-hidden="true" href="#bayes-classifier">#</a></h4>
<p><em>Bayes classifier</em> đưa ra dự đoán $y$ thuộc lớp $j$ dựa trên vector input $x_0$ mà xác suất:
</p>
$$\text{Pr}(Y=j|X=x_0)$$<p>
là lớn nhất. Đây là xác suất có điều kiện: xác suất $Y = j$ biết rằng $X = x_0$.</p>
<p>Bayes classifier có test error rate nhỏ nhất có thể, gọi là <em>Bayes error rate</em>, thường được tính qua công thức:
</p>
$$1 - E(\max_{j}\text{Pr}(Y=j|X))$$<p>
Bayes error rate giống với sai số không thể giảm được <em>irreducible error</em></p>
<h4 id="k-nearest-neighbor">K-nearest neighbor<a hidden class="anchor" aria-hidden="true" href="#k-nearest-neighbor">#</a></h4>
<p>Về mặt lý thuyết, ta luôn muốn sử dụng Bayes classifier. Tuy nhiên trên thực tế, ta không biết phân phối xác suất có điều kiện của Y theo X vì vậy tính Bayes classifier là không thể. Một trong những biện pháp thay thế là <em>K-nearest neighbors</em> (KNN) classifier.</p>
<p>Với một số thực K &gt; 0 cho trước và một điểm test $x_0$. KNN xác định tập $\mathcal{N}_0$ gồm K điểm trong tập train gần $x_0$ nhất sau đó tính:
</p>
$$\text{Pr}(Y=j|X=x_0)=\frac{1}{K}\sum_{i \in \mathcal{N}_0}I(y_i=j)$$<p>
KNN gán điểm test $x_0$ vào lớp có xác suất cao nhất tính theo công thức trên.</p>
<p>Minh họa về bài toán phân loại (hai lớp là cam và xanh). Đường màu đen là đường danh giới dự đoán của KNN, đường màu tím là của Bayes classifier (<em>Bayes decision boundary</em>).</p>
<p>Khi K=10 hai đường khá khớp với nhau:
<img alt="k = 10" loading="lazy" src="http://localhost:1313/img/2_15.jpg">
Đường K=1 quá phức tạp (flexible) trong khi đường K=100 quá đơn giản:
<img alt="k = 100 vs k = 1" loading="lazy" src="http://localhost:1313/img/2_16.jpg">
Sự thay đổi của train và test error rate khi K thay đổi (1/K tăng - tức K giảm ứng với việc tăng độ phức tạp của mô hình). Đường màu đen là Bayes error rate:
<img alt="train/ test error rate changing as K changes" loading="lazy" src="http://localhost:1313/img/2_17.jpg">
Có thể thấy K tốt nhất ứng với 1/K vào khoảng 0.1</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/statistics/">Statistics</a></li>
    </ul>
<nav class="paginav">
  <a class="next" href="http://localhost:1313/book/islp/ch2_1/">
    <span class="title">Next »</span>
    <br>
    <span>Chapter 2: Introduction to Statistical Learning (1)</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="http://localhost:1313/">Uyen&#39;s blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
