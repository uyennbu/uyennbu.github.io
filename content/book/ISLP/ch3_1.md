---
title: "Chapter 3: Linear Regression (1)"
date: 2026-02-12
draft: false
series: ["ISLP"]
tags: ["statistics", "machine learning"]
---
Bài viết nằm trong series ISLP, là series mình tóm tắt lại những gì mình đọc trong cuốn `"An Introduction to Statistical Learning with applications in Python"`.
- Trang web của quyển sách: [statlearning.com](https://www.statlearning.com/)
- Trang resources để tải file sách pdf, tải code và data:  [resources](https://www.statlearning.com/resources-python)
- Bài giảng của tác giả được cung cấp bởi đại học Stanford: [Statistical Learning with Python](https://youtube.com/playlist?list=PLoROMvodv4rPP6braWoRt5UCXYZ71GZIQ&si=wvVcrbaPvFn4x9wb)

---
Từ chương 3 này trở đi, ở đầu mỗi chương mình sẽ đặt ra một số câu hỏi quan trọng để ta cùng ngẫm nghĩ nhé. Các câu hỏi này sẽ được trả lời ở cuối chương.
1. Hồi quy tuyến tính là gì? 
2. Khi nào thì dùng hồi quy tuyến tính? Hồi quy tuyến tính giúp ta trả lời những câu hỏi gì?
3. Các tham số trong mô hình hồi quy tuyến tính được tìm bằng cách nào? Đánh giá độ chính xác ra sao (độ chính xác của tham số, độ chính xác của mô hình)?
4. Những biến thể của hồi quy tuyến tính (đó là những biến thể nào? giúp giải quyết vấn đề gì)

<!-- ## 3.0 Giới thiệu về bài toán hồi quy tuyến tính
Hồi quy tuyến tính (*linear regression*) là một cách tiếp cận rất đơn giản trong học có giám sát (*supervised learning*). Mặc dù rất đơn giản nhưng hồi quy tuyến tính lại rất hữu dụng, nhiều mô hình phức tạp trong thống kê thực chất có thể xem như sự khái quát hóa hoặc mở rộng của mô hình này.

Trở lại với ví dụ về `Advertising` trong chương 2: coi `sales` như một hàm phụ thuộc vào kinh phí quảng cáo trên `TV`, `radio` và báo. Để đề ra chiến lược kinh doanh phù hợp, ta có thể cần trà lời một số câu hỏi:
- Liệu có mối quan hệ nào giữa chi phí quảng cáo và sales không?
- Nếu có thì môi quan hệ trên có mạnh không?
- Phương tiện quảng cáo nào tác động đến doanh số? Một vài hay tất cả?
- Sự tác động của từng phương tiện quảng cáo đến doanh số như thế nào?
- Ta có thể dự đoán chính xác đến đâu doanh số tương lai?
- Mối quan hệ có tuyến tính không?
- Có mối liên hệ/ tương tác nào giữa các phương tiện quảng cáo không?

Mô hình hồi quy tuyến tính sẽ giúp ta trả lời các câu hỏi trên. -->

## 3.1 Hồi quy tuyến tính đơn (*Simple linear regression*)
Hồi quy tuyến tính giả sử rằng có mối quan hệ tuyến tính giữa $X$ và $Y$, nói cách khác, mối quan hệ giữa $X$ và $Y$ có thể viết được dưới dạng:
$$
    Y \approx \beta_0 + \beta_1X
$$
Ta nói rằng ta đang hồi quy của Y theo X (*regressing Y on X/ Y onto X*). $\beta_0$ là hệ số chặn (*intercept*), $\beta_1$ là hệ số góc (*slope*) của đường hồi quy tuyến tính.

Sau khi sử dụng dữ liệu huấn luyện để tìm $\beta_0, \beta_1$ ta được:
$$
    \hat y = \hat{\beta}_0 + \hat{\beta_1}x
$$
Kí hiệu $\hat{}$ thể hiện giá trị ước lượng.

### 3.1.1 Ước lượng các hệ số
Ta muốn tìm các hệ số $\beta_0, \beta_1$ sao cho đường thẳng tương ứng gần với các điểm dữ liệu đã biết nhất có thể: $y \approx \hat{y} = \hat{\beta}_0 + \hat{\beta_1}x$.

Có rất nhiều cách để đo sự "gần", trong đó phổ biến nhất là phương pháp bình phương tối thiểu *least square*.

***Residual sum of square***

Với $\hat{y}_i = \beta_0 + \beta_1x_i$ là dự đoán về $Y$ dựa trên giá trị thứ $i$ của $X$. Khi đó $e_i = y_i - \hat{y}_i$ là phần dư (*residual*) thứ $i$ - phần chênh lệch giữa giá quan sát và giá trị dự đoán cho $Y$ của điểm thứ i. Đại lượng tổng phần dư bình phương (*residual sum of squares*) (RSS) được định nghĩa:
$$
    \text{RSS} = e_1^2 + e_2^2 + ... + e_n^2
$$
Bình phương tối thiểu chọn $\beta_0$ và $\beta_1$ để tối thiểu hóa RSS. Bằng các biến đổi trong giải tích, ta thu được:
$$
    \hat{\beta_1} = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n}(x_i - \bar{x})^2}\\
    \hat{\beta_0} = \bar{y} - \hat{\beta_1}\bar{x}
$$
Trong đó $\bar{y} = \frac{1}{n}\sum_{i=1}{n}y_i$ và $\bar{x} = \frac{1}{n}\sum_{i=1}{n}x_i$ là trung bình mẫu.

Đường bình phương tối thiểu cho ví dụ về chi phí quảng cáo:
![ls_line_avtising](/img/Chapter3/3_1.jpg)

### 3.1.2. Đánh giá độ chính xác của các hệ số đã ước lượng
Ở mục này có hai ý chính ta cần quan tâm:
- Sai số chuẩn của ước lượng và khoảng tin cậy
- Liệu X có thật sự có tác động lên Y hay mô hình hồi quy tuyến tính có thật sự ý nghĩa không.

**Vấn đề 1. Khoảng tin cậy cho hệ số của mô hình**

Nếu như mối quan hệ giữa X và Y thực sự có thể biểu diễn xấp xỉ bằng đường thẳng, thì đường thẳng này được gọi là đường hồi quy tổng thể (*population regression line*), là đường thẳng tốt nhất có thể dùng để biểu diễn quan hệ giữa X và Y:
$$
    Y = \beta_0 + \beta_1 X + \epsilon
$$
Trong đó, hệ số chặn $\beta_0$ là kỳ vọng của Y khi X = 0, $\beta_1$ cho biết lượng tăng trung bình của Y khi X tăng 1 đơn vị và $\epsilon$ là sai số (đến từ việc mối quan hệ thực sự có thể không phải là tuyến tính và cũng đến từ những tác động bên ngoài khác đến Y). Ta thường giả định rằng $\epsilon$ độc lập với X.

Đường thẳng với hệ số nhận được từ phương pháp bình phương tối thiểu như phần trên được gọi là đường bình phương tối thiểu (*least square line*).

Đường hồi quy tổng thể là quy nhất, trong khi đường least squares thì lại có rất nhiều do được tính toán từ mẫu. Mẫu khác nhau sẽ có những đường least squares khác nhau. Trong hình dưới đây, đường màu đỏ là đường hồi quy tổng thể, màu xanh đậm là đường least squares ta tính được ở trên còn các đường xanh nhạt là các đường least squares ứng với các bộ dữ liệu khác.
![ls_vs_population](/img/Chapter3/3_3.png)

Có thể thấy luôn có sự khác nhau giữa các đường least squares và đường hồi quy tổng thể. Tuy nhiên least squares là ước lượng không chệch, nghĩa là nếu ta lấy trung bình các ước lượng cho $\hat{\beta_0}$ và $\hat{\beta_1}$ trên một lượng lớn các tập dữ liệu khác nhau thì giá trị $\hat{\beta_0}, \hat{\beta_1}$ này sẽ rất gần với giá trị của $\beta_0$ và $\beta_1$. Ta có thể thấy điều này trong hình bên phải. 

Sai số chuẩn (*standard error*) cho biết ước lượng $\hat{\beta_0}$ và $\hat{\beta_1}$ sai khác như thế nào so với giá trị thực của $\beta_0$ và $\beta_1$. Với least squares sai số chuẩn được tính theo công thức:
$$
    SE(\hat{\beta_0})^2 = \sigma^2[\frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^{n}(x_i - \bar{x})^2}], \quad SE(\hat{\beta_1})^2 = \frac{\sigma^2}{\sum_{i=1}^{n}(x_i - \bar{x})^2}
$$
Trong đó $\sigma^2 = Var(\epsilon)$. Khi sử dụng công thức này ta đã giả định rằng $\epsilon$ có cùng phương sai và không tương quan với nhau. Điều này không đúng (có thể xem ở hình <!--SOS--> trước) tuy nhiên đây vẫn là một cách xấp xỉ tốt.

Theo công thức này, $SE(\hat{\beta_1})$ càng nhỏ khi $x_i$ càng phân tán (dữ liệu *leverage*), để dễ hình dung ta có thể nghĩ đến việc tìm hệ số góc cho đường thẳng hồi quy khi dữ liệu tụ lại thành một đám tròn, so với khi dữ liệu tản dẹt ra. 

Với công thức tính $SE(\hat{\beta_0})$ thì nếu $\bar{x} = 0$, $SE(\hat{\beta_1}) = \sigma^2/n = SE(\hat{\mu})$ là phương sai của ước lượng cho kì vọng của Y dựa trên trung bình mẫu. Chú ý rằng $\beta_0$ là kỳ vọng của $Y$ khi X = 0.

Trên thực tế, ta không biết $\sigma^2$ nhưng ta có thể ước lượng cho $\sigma$ bằng RSE (*residual standard error*, sai số chuẩn phần dư) dựa trên dữ liệu theo công thức $RSE = \sqrt{RSS/(n-2)}$.

Sai số chuẩn có thể được dùng để tính khoảng tin cậy *confident interval*. Khoảng tin cậy 95% là khoảng mà có xác suất 95% rằng giá trị thực sẽ nằm trong nó. Trong hồi quy tuyến tính, khoảng tin cậy 95% cho $\beta_1$ được xấp xỉ bởi:
$$
    \hat{\beta_1} \pm 2 \cdot SE(\hat{\beta_1}) \\
    \text{hay } [\hat{\beta_1} - 2 \cdot SE(\hat{\beta_1}), \hat{\beta_1} + 2 \cdot SE(\hat{\beta_1})] 
$$
Tương tự với $\beta_0$.

Trong bài toán về sales và chi phí quảng cáo trên, khoảng tin cậy 95% cho $\beta_0$ và $\beta_1$ lần lượt là `[6.130, 7.935]` và `[0.042, 0.053]`. Điều này có nghĩa là nếu không có quảng cáo thì trung bình lượng sales sẽ là 6,130 đến 7,935 sản phẩm. Thêm vào đó, với mỗi 1000 đô chi thêm cho quảng cáo trên TV thì doanh số tăng 42 đến 53 đơn vị sản phẩm.

**Vấn đề 2: Kiểm định về hệ số của mô hình**

Một câu hỏi ta cần quan tâm là, liệu X có thật sự có tác động lên Y hay không? Hay liệu có thật sự có mối quan hệ nào giữa X và Y hay không. Vì nếu không thì mô hình có hệ số góc = 0, hay $Y = \beta_0 + \epsilon$, X không có ảnh hưởng gì đến Y.

Ta cần kiểm định giả thiết (*null hypothesis*):
- $H_0$: không có mối quan hệ giữa X và Y hay $\beta_1 = 0$
với đối thiết (*alternative hypothesis*):
- $H_a$: có mỗi quan hệ nào đó giữa X và Y, hay $\beta_1 \neq 0$

Ta sẽ bác bỏ $H_0$ nếu hệ số đủ xa so vơi 0. Ta kiểm tra điều này bằng cách so khoảng cách giữa $\beta_1$ và 0 với SE($\hat{\beta_1}$). Nếu sai số của ước lượng là nhỏ, thì giá trị của $\hat{\beta_1}$ dù rất nhỏ nhưng cũng đáng kể. Ngược lại, nếu sai số ước lượng là lớn thì $\hat{\beta_1}$ cũng phải lớn tương ứng. 

Ta sử dụng test thống kê T, giúp đo xem $\hat{\beta_1}$ cách 0 bao nhiêu độ lệch chuẩn.:
$$
    t = \frac{\hat{\beta_1} - 0}{SE(\hat{\beta_1})}
$$
Nếu $H_0$ đúng, nghĩa là không có mối quan hệ giữa X và Y thì đại lượng t tuân theo phân phối t-student với $n-2$ bậc tự do, phân phối này xâp xỉ phân phối chuẩn tắc nếu n > 30. 

Đại lượng p-value là xác suất để thu được giá trị t như ta đã tính được ở trên nếu $H_0$ đúng. Nghĩa là, để kiểm định xem $H_0$ có đúng không, ta giả sử rằng nó đúng, khi đó ta sẽ xấp xỉ được phân phối của t và từ đó tính xác suất nhận được giá trị t như ta đã tính. Nếu xác suất này đủ nhỏ, ta sẽ bác bỏ $H_0$ và ngược lại.

Vậy ta sẽ bác bỏ $H_0$, hay ta nói rằng X và Y có mối quan hệ nào đó nếu p-value đủ nhỏ (thường là nhỏ hơn 0.05, ứng với độ tin cậy 95%).
<!--Bảng và nhận xét-->
### 3.1.3. Đánh giá độ chính xác của mô hình
Sau khi đã bác bỏ $H_0$, điều tiếp theo ta muốn làm đó là đánh giá múc độ phù hợp với dữ liệu của mô hình. Ta có một số hệ số để đánh giá như sau:

**Residual Standard Error**

Kể cả khi ta biết được đường hồi quy thật sự thì dự đoán của mô hình vẫn luôn có sai số $\epsilon$ do nhiều tác động khác bên ngoài. 

RSE là ước lượng cho sai số chuẩn của $\epsilon$, hay nó là lượng chênh lệch trung bình của Y so với đường hồi quy thực sự (*true regression line*):
$$
    RSE = \sqrt{\frac{1}{n - 2}RSS} = \sqrt{\frac{1}{n-2}\sum_{i=1}^{n}(y_i - \hat{y_i})^2}
$$

RSE còn có thể coi như đại lượng đo lường độ thiếu phù hợp của mô hình *lack of fit* so với dữ liệu.

Thông thường, RSE càng lớn tức là mô hình không thật sự phù hợp tốt với dữ liệu.

**$R^2$ statistic**

RSE cho đo lường độ lệch tuyệt đối do đó độ lớn của nó còn phụ thuộc vào đơn vị của Y, làm việc so sánh giữa các mô hình gặp khó khăn hơn.

$R^2$ là một cách khác để đo độ phù hợp, do được tính theo tỉ lệ nên nó không phụ thuộc vào đơn vị của Y và luôn có giá trị trong khoảng (0,1), được xác định bởi:
$$
    R^2 = \frac{TSS - RSS}{TSS} = 1 - \frac{RSS}{TSS}
$$
Trong đó TSS = $\sum_{i=1}^{n}(y_i - \bar{y})^2$ là tổng bình phương trung bình *total sum of squares*, đo lường tổng phương sai của phản hồi Y, hay sự biến động của Y trước khi sử dụng hồi quy. Ngược lại, RSS đo lường độ biến động mà không giải thích được sau khi áp dụng hồi quy. 

Do đó, $R^2$ đo lường tỉ lệ độ biến động *variability* trong Y mà có thể giải thích được bằng X.

$R^2$ gần với 1 nghĩa là có một tỉ lệ lớn của sự biến động được giải thích bởi hồi quy. Còn $R^2$ gần với 0 nghĩa là hồi quy không thể giải thích được nhiều sự biến động trong phản hồi Y, điều này có thể do mô hình tuyến tính là sai hoặc phương sai của sai số $\sigma^2$ là cao, hoặc cả hai.

*$R^2$ vs correlation*

Hệ số tương quan *correlation*: r = Cor(X,Y) cũng giúp đo lường mối quan hệ tuyến tính giữa X và Y, được tính bởi:
$$
    Cor(X,Y) = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^{n}(x_i-\bar{x})^2}\sqrt{\sum_{i=1}^{n}(y_i-\bar{y})^2}}
$$
Trong hồi quy tuyến tính đơn ta có thể dùng r thay cho $R^2$, và ta cũng chứng minh được rằng $R^2 = r^2$. Tuy nhiên với hồi quy tuyến tính đa biến, ta không dùng r mà dùng $R^2$ do r chỉ dựa trên cặp 2 biến chứ không phải giữa một lượng lớn các biến.

--- 
Bài viết của mình có thể còn nhiều thiếu sót, mình rất vui nếu được nhận góp ý từ bạn đọc để bài viết hoàn thiện và trở nên tốt hơn. Chúc bạn một ngày tốt lành ☘️

Email của mình : uyennguyen.nbu@gmail.com