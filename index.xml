<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Uyen's blog</title><link>https://nguyen451.github.io/</link><description>Recent content on Uyen's blog</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Fri, 09 Jan 2026 00:00:00 +0000</lastBuildDate><atom:link href="https://nguyen451.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>Nh·ªØng l·ªùi ƒë·∫ßu ti√™n</title><link>https://nguyen451.github.io/myjourney/first_lines/</link><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate><guid>https://nguyen451.github.io/myjourney/first_lines/</guid><description>&lt;p&gt;B√†i vi·∫øt n√†y thu·ªôc series &amp;ldquo;Study in HUS&amp;rdquo;, n∆°i m√¨nh chia s·∫ª nh·ªØng c·∫£m nh·∫≠n v√† suy nghƒ© c·ªßa m√¨nh trong qu√° tr√¨nh h·ªçc t·∫≠p l·∫°i HUS.&lt;/p&gt;
&lt;h1 id="nh√¨n-l·∫°i"&gt;Nh√¨n l·∫°i&lt;/h1&gt;
&lt;p&gt;H√¥m nay l√† 9/1/2026, m√¨nh ch∆∞a thi xong h·∫øt h·ªçc k√¨ 1 nh∆∞ng c≈©ng c√≤n 1 m√¥n n·ªØa th√¥i, c≈©ng coi nh∆∞ ƒë√£ ho√†n th√†nh h·ªçc k√¨ n√†y r·ªìi. V·∫≠y l√† m√¨nh ƒë√£ tr·∫£i qua 3 h·ªçc k√¨ t·∫°i HUS, c≈©ng m·ªõi ch·ªâ l√† 3/8 k√¨ h·ªçc, n·∫øu m√¨nh h·ªçc ƒë·ªß 8 k√¨. Nh∆∞ng m√† c√≥ th·ªÉ m√¨nh s·∫Ω kh√¥ng d√πng h·∫øt 8 k√¨ n√†y, h√¥m tr∆∞·ªõc xem l·∫°i m√¨nh th·∫•y ch·∫Øc m√¨nh ch·ªâ c·∫ßn 6 - 7 k√¨ l√† xong r·ªìi, v·∫≠y m√¨nh c≈©ng coi nh∆∞ ƒë√£ ƒëi ƒë∆∞·ª£c n·ª≠a ƒë∆∞·ªùng r·ªìi, s·∫Øp ra tr∆∞·ªùng (h∆°i s·ªõm nh∆∞ng d·∫°o n√†y m√¨nh th·∫≠t s·ª± c√≥ c·∫£m gi√°c ·∫•y üòÇ)&lt;/p&gt;</description></item><item><title>Chapter 4: Classification</title><link>https://nguyen451.github.io/book/islp/ch4_classification/</link><pubDate>Tue, 25 Nov 2025 00:00:00 +0000</pubDate><guid>https://nguyen451.github.io/book/islp/ch4_classification/</guid><description>&lt;p&gt;As we all know, there are two main types of data: quantitative and qualitative. The linear regression models in previous chapters help us to predict quantitative responses. But what about qualitative ones?&lt;/p&gt;
&lt;p&gt;The task of predicting qualitative responses is known as &lt;em&gt;classification&lt;/em&gt;. In this chapter, we will walk through the base idea of classification and some of the most basic and common classification models.&lt;/p&gt;
&lt;p&gt;Some examples of classification problems include:&lt;/p&gt;</description></item><item><title>Chapter 1: Introduction and examples</title><link>https://nguyen451.github.io/book/firstcoursebayes/ch1_intro/</link><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate><guid>https://nguyen451.github.io/book/firstcoursebayes/ch1_intro/</guid><description>&lt;h1 id="chapter-1-introduction-and-examples"&gt;Chapter 1: Introduction and examples&lt;/h1&gt;
&lt;h2 id="what-is-bayesian-statistics"&gt;What is Bayesian Statistics?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Bayesian inference is the process of &lt;strong&gt;inductive&lt;/strong&gt; learning via Bayes&amp;rsquo; rule.&lt;/li&gt;
&lt;li&gt;Bayes rule does not tell us what our beliefs should be, it tells us how should they change after seeing new information.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="notation"&gt;Notation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;parameters: $\theta$ (we call it parameter because we calculate y from it, in normal cases)&lt;/li&gt;
&lt;li&gt;dataset: $y$ (sample of the population)&lt;/li&gt;
&lt;li&gt;sample space : $\mathcal{Y}$ (all possible datasets)&lt;/li&gt;
&lt;li&gt;parameter space : $\Theta$ (all possible parameters)&lt;/li&gt;
&lt;li&gt;prior distribution: $p(\theta)$ (for each $\theta \in \Theta$, $p(\theta)$ is our belief that $\theta$ represents the true population chracteristic)&lt;/li&gt;
&lt;li&gt;sampling method: $p(y|\theta)$ (our belief that $y$ should be observed if $\theta$ is known)&lt;/li&gt;
&lt;li&gt;posterior distribution: $p(\theta|y)$ (our belief that $\theta$ is the true population characteristic after observing $y$)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="bayes-rule"&gt;Bayes&amp;rsquo; Rule&lt;/h2&gt;
$$p(\theta|y) = \frac{p(y|\theta)p(\theta)}{\int_\Theta p(y|\tilde{\theta})p(\tilde{\theta})d\tilde{\theta}}$$&lt;h2 id="examples"&gt;Examples&lt;/h2&gt;
&lt;h3 id="using-bayes-inference-to-estimate-posterior-probability"&gt;Using Bayes inference to estimate posterior probability&lt;/h3&gt;
&lt;h3 id="using-bayes-inference-to-estimate-parameter-in-a-predictive-model"&gt;Using Bayes inference to estimate parameter in a predictive model&lt;/h3&gt;</description></item><item><title>Chapter 2: Belief, Probbility, and Exchangeability</title><link>https://nguyen451.github.io/book/firstcoursebayes/ch2_belief_prob_exchangeability/</link><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate><guid>https://nguyen451.github.io/book/firstcoursebayes/ch2_belief_prob_exchangeability/</guid><description>&lt;h2 id="belief-function-and-probability"&gt;Belief function and Probability&lt;/h2&gt;
&lt;h2 id="events-partitions-and-bayes-rule"&gt;Events, partitions, and Bayes&amp;rsquo; rule&lt;/h2&gt;
&lt;h2 id="independence"&gt;Independence&lt;/h2&gt;
&lt;h2 id="random-variables"&gt;Random variables&lt;/h2&gt;
&lt;h3 id="discrete-random-variables"&gt;Discrete random variables&lt;/h3&gt;
&lt;h3 id="continuous-random-variables"&gt;Continuous random variables&lt;/h3&gt;
&lt;h2 id="joint-distributions"&gt;Joint distributions&lt;/h2&gt;
&lt;h2 id="independent-random-variables"&gt;Independent random variables&lt;/h2&gt;
&lt;h2 id="exchangeability"&gt;Exchangeability&lt;/h2&gt;
&lt;h3 id="de-finettis-theorem"&gt;De Finetti&amp;rsquo;s theorem&lt;/h3&gt;</description></item><item><title>Chapter 2: Introduction to Statistical Learning</title><link>https://nguyen451.github.io/book/islp/ch2_statistical_learning/</link><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate><guid>https://nguyen451.github.io/book/islp/ch2_statistical_learning/</guid><description>&lt;h2 id="what-is-statistical-learning"&gt;What is Statistical Learning?&lt;/h2&gt;</description></item><item><title>Chapter 3: One-parameter models</title><link>https://nguyen451.github.io/book/firstcoursebayes/ch3_one_prarameter_models/</link><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate><guid>https://nguyen451.github.io/book/firstcoursebayes/ch3_one_prarameter_models/</guid><description>&lt;h2 id="binomial-model"&gt;Binomial model&lt;/h2&gt;
&lt;h2 id="poisson-model"&gt;Poisson model&lt;/h2&gt;
&lt;h2 id="exponential-model"&gt;Exponential model&lt;/h2&gt;</description></item><item><title>Second</title><link>https://nguyen451.github.io/demo/second/</link><pubDate>Mon, 15 Jul 2024 00:00:00 +0000</pubDate><guid>https://nguyen451.github.io/demo/second/</guid><description>&lt;h1 id="second-demo"&gt;Second Demo&lt;/h1&gt;
&lt;p&gt;This is the content of the second demo post.&lt;/p&gt;</description></item><item><title>First Demo</title><link>https://nguyen451.github.io/demo/first/</link><pubDate>Sat, 15 Jun 2024 00:00:00 +0000</pubDate><guid>https://nguyen451.github.io/demo/first/</guid><description>&lt;h1 id="first-demo"&gt;First Demo&lt;/h1&gt;
&lt;p&gt;This is the content of the first demo post.&lt;/p&gt;</description></item><item><title>About Me</title><link>https://nguyen451.github.io/about/</link><pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate><guid>https://nguyen451.github.io/about/</guid><description>&lt;h1 id="about-me"&gt;About Me&lt;/h1&gt;
&lt;p&gt;M√¨nh t√™n l√† Nguy·ªÖn B√≠ch Uy√™n, ƒëang l√† sinh vi√™n ng√†nh khoa h·ªçc d·ªØ li·ªáu t·∫°i tr∆∞·ªùng ƒê·∫°i h·ªçc Khoa h·ªçc T·ª± nhi√™n, ƒê·∫°i h·ªçc Qu·ªëc gia H√† N·ªôi. M√¨nh vi·∫øt blog n√†y ƒë·ªÉ chia s·∫ª nh·ªØng ki·∫øn th·ª©c m√¨nh h·ªçc ƒë∆∞·ª£c v·ªÅ th·ªëng k√™ v√† h·ªçc m√°y, c≈©ng nh∆∞ nh·ªØng tr·∫£i nghi·ªám c·ªßa m√¨nh trong qu√° tr√¨nh h·ªçc t·∫≠p t·∫°i HUS.&lt;/p&gt;</description></item><item><title>Archives</title><link>https://nguyen451.github.io/archives/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nguyen451.github.io/archives/</guid><description>A collection of all posts organized by month and year.</description></item></channel></rss>